{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756cc386",
   "metadata": {},
   "source": [
    "# Assigment 2 - Task 1\n",
    "\n",
    "##### Name: Jaimis Arvindbhai Miyani\n",
    "    \n",
    "##### Student ID: 400551743\n",
    "    \n",
    "##### MacID: miyanij@mcmaster.ca\n",
    "    \n",
    "##### Subject: SEP 775 - Introduction to Computational Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff28d5e",
   "metadata": {},
   "source": [
    "# RNN-Based Text Generation\n",
    "### Objectives:\n",
    "Implement a simple RNN for text generation to deepen your understanding of how re- current neural networks can be used to model sequences and generate text based on learned patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028cc54a",
   "metadata": {},
   "source": [
    "### 1. RNN Model Implementation\n",
    "#### Implement a basic Recurrent Neural Network model from scratch using PyTorch or TensorFlow. Your model should include an embedding layer, at least one RNN layer, and a fully connected layer for output. Refer to the \"Recurrent Neural Networks (RNN)\"section of the lectures for guidance on the architecture.\n",
    "#### Use the\"Long Short-Term Memory RNNs(LSTMs)\"section as a reference to enhance your model with LSTM cells to improve its ability to capture long-term dependencies in text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babc64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, bidirectional=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # If bidirectional, concatenate the final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        output = self.fc(hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abd6d1",
   "metadata": {},
   "source": [
    "### 2. Dataset Preparation\n",
    "#### Select a small text dataset for training your model. This could be a collection of poems, song lyrics, or any text of your choice. Preprocess the data by tokenizing the text into sequences and converting them into numerical format suitable for training your RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e9853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Tokens ---------------\n",
      "\n",
      "['uh', 'huh', 'uh', 'huh', 'yeah', 'rihanna', 'uh', 'huh', 'uh', 'huh', 'good', 'girl', 'gone', 'bad', 'uh', 'huh', 'uh', 'huh', 'take', 'three', 'action', 'uh', 'huh', 'uh', 'huh', 'hov', 'no', 'clouds', 'in', 'my', 'stones', 'let', 'it', 'rain', 'i', 'hydroplane', 'in', 'the', 'bank', 'comin', 'down', 'like', 'the', 'dow', 'jones', 'when', 'the', 'clouds', 'come', 'we', 'gone', 'we', 'roc', 'a', 'fella', 'we', 'fly', 'higher', 'than', 'weather', 'in', 'g5', 's', 'or', 'better', 'you', 'know', 'me', 'you', 'know', 'me', 'in', 'anticipation', 'for', 'precipitation', 'stack', 'chips', 'for', 'the', 'rainy', 'day', 'jay', 'rain', 'man', 'is', 'back', 'with', 'little', 'ms', 'sunshine', 'rihanna', 'where', 'you', 'at', 'you', 'have', 'my', 'heart', 'and', 'we', 'll', 'never', 'be', 'worlds', 'apart', 'maybe', 'in', 'magazines', 'but', 'you', 'll', 'still', 'be', 'my', 'star', 'baby', 'cause', 'in', 'the', 'dark', 'you', 'can', 't', 'see', 'shiny', 'cars', 'and', 'that', 's', 'when', 'you', 'need', 'me', 'there', 'with', 'you', 'i', 'll', 'always', 'share', 'because', 'when', 'the', 'sun', 'shines', 'we', 'll', 'shine', 'together', 'told', 'you', 'i', 'll', 'be', 'here', 'forever', 'said', 'i', 'll', 'always', 'be', 'your', 'friend', 'took', 'an', 'oath', 'i', 'ma', 'stick', 'it', 'out', 'to', 'the', 'end', 'now', 'that', 'it', 's', 'raining', 'more', 'than', 'ever', 'know', 'that', 'we', 'll', 'still', 'have', 'each', 'other', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'these', 'fancy', 'things', 'will', 'never', 'come', 'in', 'between', 'you', 're', 'part', 'of', 'my', 'entity', 'here', 'for', 'infinity', 'when', 'the', 'war', 'has', 'took', 'its', 'part', 'when', 'the', 'world', 'has', 'dealt', 'its', 'cards', 'if', 'the', 'hand', 'is', 'hard', 'together', 'we', 'll', 'mend', 'your', 'heart', 'because', 'when', 'the', 'sun', 'shines', 'we', 'shine', 'together', 'told', 'you', 'i', 'll', 'be', 'here', 'forever', 'said', 'i', 'll', 'always', 'be', 'your', 'friend', 'took', 'an', 'oath', 'i', 'ma', 'stick', 'it', 'out', 'to', 'the', 'end', 'now', 'that', 'it', 's', 'raining', 'more', 'than', 'ever', 'know', 'that', 'we', 'll', 'still', 'have', 'each', 'other', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'you', 'can', 'run', 'into', 'my', 'arms', 'it', 's', 'okay', 'don', 't', 'be', 'alarmed', 'come', 'into', 'me', 'there', 's', 'no', 'distance', 'in', 'between', 'our', 'love', 'so', 'gon', 'and', 'let', 'the', 'rain', 'pour', 'i', 'll', 'be', 'all', 'you', 'need', 'and', 'more', 'because', 'when', 'the', 'sun', 'shines', 'we', 'shine', 'together', 'told', 'you', 'i', 'll', 'be', 'here', 'forever', 'said', 'i', 'll', 'always', 'be', 'your', 'friend', 'took', 'an', 'oath', 'i', 'ma', 'stick', 'it', 'out', 'to', 'the', 'end', 'now', 'that', 'it', 's', 'raining', 'more', 'than', 'ever', 'know', 'that', 'we', 'll', 'still', 'have', 'each', 'other', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'you', 'can', 'stand', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'under', 'my', 'umbrella', 'ella', 'ella', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'it', 's', 'raining', 'raining', 'ooh', 'baby', 'it', 's', 'raining', 'raining', 'baby', 'come', 'into', 'me', 'come', 'into', 'me', 'it', 's', 'raining', 'raining', 'ooh', 'baby', 'it', 's', 'raining', 'raining', 'you', 'can', 'always', 'come', 'into', 'me', 'come', 'into', 'me', 'it', 's', 'pouring', 'rain', 'it', 's', 'pouring', 'rain', 'come', 'into', 'me', 'come', 'into', 'me', 'it', 's', 'pouring', 'rain', 'it', 's', 'pouring', 'rain', 'come', 'into', 'me']\n",
      "\n",
      "\n",
      "------------- Vocabulary -------------\n",
      "\n",
      "{'uh': 0, 'huh': 1, 'yeah': 2, 'rihanna': 3, 'good': 4, 'girl': 5, 'gone': 6, 'bad': 7, 'take': 8, 'three': 9, 'action': 10, 'hov': 11, 'no': 12, 'clouds': 13, 'in': 14, 'my': 15, 'stones': 16, 'let': 17, 'it': 18, 'rain': 19, 'i': 20, 'hydroplane': 21, 'the': 22, 'bank': 23, 'comin': 24, 'down': 25, 'like': 26, 'dow': 27, 'jones': 28, 'when': 29, 'come': 30, 'we': 31, 'roc': 32, 'a': 33, 'fella': 34, 'fly': 35, 'higher': 36, 'than': 37, 'weather': 38, 'g5': 39, 's': 40, 'or': 41, 'better': 42, 'you': 43, 'know': 44, 'me': 45, 'anticipation': 46, 'for': 47, 'precipitation': 48, 'stack': 49, 'chips': 50, 'rainy': 51, 'day': 52, 'jay': 53, 'man': 54, 'is': 55, 'back': 56, 'with': 57, 'little': 58, 'ms': 59, 'sunshine': 60, 'where': 61, 'at': 62, 'have': 63, 'heart': 64, 'and': 65, 'll': 66, 'never': 67, 'be': 68, 'worlds': 69, 'apart': 70, 'maybe': 71, 'magazines': 72, 'but': 73, 'still': 74, 'star': 75, 'baby': 76, 'cause': 77, 'dark': 78, 'can': 79, 't': 80, 'see': 81, 'shiny': 82, 'cars': 83, 'that': 84, 'need': 85, 'there': 86, 'always': 87, 'share': 88, 'because': 89, 'sun': 90, 'shines': 91, 'shine': 92, 'together': 93, 'told': 94, 'here': 95, 'forever': 96, 'said': 97, 'your': 98, 'friend': 99, 'took': 100, 'an': 101, 'oath': 102, 'ma': 103, 'stick': 104, 'out': 105, 'to': 106, 'end': 107, 'now': 108, 'raining': 109, 'more': 110, 'ever': 111, 'each': 112, 'other': 113, 'stand': 114, 'under': 115, 'umbrella': 116, 'ella': 117, 'eh': 118, 'these': 119, 'fancy': 120, 'things': 121, 'will': 122, 'between': 123, 're': 124, 'part': 125, 'of': 126, 'entity': 127, 'infinity': 128, 'war': 129, 'has': 130, 'its': 131, 'world': 132, 'dealt': 133, 'cards': 134, 'if': 135, 'hand': 136, 'hard': 137, 'mend': 138, 'run': 139, 'into': 140, 'arms': 141, 'okay': 142, 'don': 143, 'alarmed': 144, 'distance': 145, 'our': 146, 'love': 147, 'so': 148, 'gon': 149, 'pour': 150, 'all': 151, 'ooh': 152, 'pouring': 153}\n",
      "\n",
      "\n",
      "----------- Numerical text -----------\n",
      "\n",
      "[0, 1, 0, 1, 2, 3, 0, 1, 0, 1, 4, 5, 6, 7, 0, 1, 0, 1, 8, 9, 10, 0, 1, 0, 1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 14, 22, 23, 24, 25, 26, 22, 27, 28, 29, 22, 13, 30, 31, 6, 31, 32, 33, 34, 31, 35, 36, 37, 38, 14, 39, 40, 41, 42, 43, 44, 45, 43, 44, 45, 14, 46, 47, 48, 49, 50, 47, 22, 51, 52, 53, 19, 54, 55, 56, 57, 58, 59, 60, 3, 61, 43, 62, 43, 63, 15, 64, 65, 31, 66, 67, 68, 69, 70, 71, 14, 72, 73, 43, 66, 74, 68, 15, 75, 76, 77, 14, 22, 78, 43, 79, 80, 81, 82, 83, 65, 84, 40, 29, 43, 85, 45, 86, 57, 43, 20, 66, 87, 88, 89, 29, 22, 90, 91, 31, 66, 92, 93, 94, 43, 20, 66, 68, 95, 96, 97, 20, 66, 87, 68, 98, 99, 100, 101, 102, 20, 103, 104, 18, 105, 106, 22, 107, 108, 84, 18, 40, 109, 110, 37, 111, 44, 84, 31, 66, 74, 63, 112, 113, 43, 79, 114, 115, 15, 116, 43, 79, 114, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 118, 118, 118, 119, 120, 121, 122, 67, 30, 14, 123, 43, 124, 125, 126, 15, 127, 95, 47, 128, 29, 22, 129, 130, 100, 131, 125, 29, 22, 132, 130, 133, 131, 134, 135, 22, 136, 55, 137, 93, 31, 66, 138, 98, 64, 89, 29, 22, 90, 91, 31, 92, 93, 94, 43, 20, 66, 68, 95, 96, 97, 20, 66, 87, 68, 98, 99, 100, 101, 102, 20, 103, 104, 18, 105, 106, 22, 107, 108, 84, 18, 40, 109, 110, 37, 111, 44, 84, 31, 66, 74, 63, 112, 113, 43, 79, 114, 115, 15, 116, 43, 79, 114, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 118, 118, 118, 43, 79, 139, 140, 15, 141, 18, 40, 142, 143, 80, 68, 144, 30, 140, 45, 86, 40, 12, 145, 14, 123, 146, 147, 148, 149, 65, 17, 22, 19, 150, 20, 66, 68, 151, 43, 85, 65, 110, 89, 29, 22, 90, 91, 31, 92, 93, 94, 43, 20, 66, 68, 95, 96, 97, 20, 66, 87, 68, 98, 99, 100, 101, 102, 20, 103, 104, 18, 105, 106, 22, 107, 108, 84, 18, 40, 109, 110, 37, 111, 44, 84, 31, 66, 74, 63, 112, 113, 43, 79, 114, 115, 15, 116, 43, 79, 114, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 115, 15, 116, 117, 117, 118, 118, 118, 118, 118, 118, 18, 40, 109, 109, 152, 76, 18, 40, 109, 109, 76, 30, 140, 45, 30, 140, 45, 18, 40, 109, 109, 152, 76, 18, 40, 109, 109, 43, 79, 87, 30, 140, 45, 30, 140, 45, 18, 40, 153, 19, 18, 40, 153, 19, 30, 140, 45, 30, 140, 45, 18, 40, 153, 19, 18, 40, 153, 19, 30, 140, 45]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text by splitting on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "def create_vocab(tokenized_text):\n",
    "    # Create vocabulary (map tokens to indices)\n",
    "    vocab = {}\n",
    "    for token in tokenized_text:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def numerify_text(tokenized_text, vocab):\n",
    "    # Convert tokens to numerical indices using vocabulary\n",
    "    numerical_text = [vocab[token] for token in tokenized_text]\n",
    "    return numerical_text\n",
    "\n",
    "# Your choice of song lyrics\n",
    "song_lyrics = \"\"\"\n",
    "Uh huh, uh huh (yeah, Rihanna)\n",
    "Uh huh, uh huh (Good Girl Gone Bad)\n",
    "Uh huh, uh huh (take three, action)\n",
    "Uh huh, uh huh (Hov)\n",
    "\n",
    "No clouds in my stones\n",
    "Let it rain, I hydroplane in the bank\n",
    "Comin' down like the Dow Jones\n",
    "When the clouds come, we gone\n",
    "We Roc-A-Fella\n",
    "We fly higher than weather\n",
    "In G5's or better\n",
    "You know me (you know me)\n",
    "In anticipation for precipitation stack chips for the rainy day\n",
    "Jay, Rain Man is back\n",
    "With Little Ms. Sunshine, Rihanna, where you at?\n",
    "\n",
    "You have my heart\n",
    "And we'll never be worlds apart\n",
    "Maybe in magazines\n",
    "But you'll still be my star\n",
    "Baby, 'cause in the dark\n",
    "You can't see shiny cars\n",
    "And that's when you need me there\n",
    "With you I'll always share\n",
    "Because\n",
    "\n",
    "When the sun shines, we'll shine together\n",
    "Told you I'll be here forever\n",
    "Said I'll always be your friend\n",
    "Took an oath, I'ma stick it out to the end\n",
    "Now that it's raining more than ever\n",
    "Know that we'll still have each other\n",
    "You can stand under my umbrella\n",
    "You can stand under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh, eh, eh-eh\n",
    "\n",
    "These fancy things will never come in between\n",
    "You're part of my entity, here for infinity\n",
    "When the war has took its part\n",
    "When the world has dealt its cards\n",
    "If the hand is hard\n",
    "Together we'll mend your heart\n",
    "Because\n",
    "\n",
    "When the sun shines, we shine together\n",
    "Told you I'll be here forever\n",
    "Said I'll always be your friend\n",
    "Took an oath, I'ma stick it out to the end\n",
    "Now that it's raining more than ever\n",
    "Know that we'll still have each other\n",
    "You can stand under my umbrella\n",
    "You can stand under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh, eh, eh-eh\n",
    "\n",
    "You can run into my arms\n",
    "It's okay, don't be alarmed\n",
    "Come into me (there's no distance in between our love)\n",
    "So gon' and let the rain pour\n",
    "I'll be all you need and more\n",
    "Because\n",
    "\n",
    "When the sun shines, we shine together\n",
    "Told you I'll be here forever\n",
    "Said I'll always be your friend\n",
    "Took an oath, I'ma stick it out to the end\n",
    "Now that it's raining more than ever\n",
    "Know that we'll still have each other\n",
    "You can stand under my umbrella\n",
    "You can stand under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh\n",
    "Under my umbrella, ella, ella, eh, eh, eh, eh, eh-eh\n",
    "\n",
    "It's raining, raining\n",
    "Ooh, baby, it's raining, raining\n",
    "Baby, come into me\n",
    "Come into me\n",
    "It's raining, raining\n",
    "Ooh, baby, it's raining, raining\n",
    "You can always come into me\n",
    "Come into me\n",
    "It's pouring rain\n",
    "It's pouring rain\n",
    "Come into me\n",
    "Come into me\n",
    "It's pouring rain\n",
    "It's pouring rain, come into me\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Preprocess the text\n",
    "tokens = preprocess_text(song_lyrics)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = create_vocab(tokens)\n",
    "\n",
    "# Numerify the text\n",
    "numerical_text = numerify_text(tokens, vocab)\n",
    "\n",
    "# Print tokens, vocabulary, and numerical text\n",
    "print(f\"--------------- Tokens ---------------\\n\\n{tokens}\")\n",
    "print(f\"\\n\\n------------- Vocabulary -------------\\n\\n{vocab}\")\n",
    "print(f\"\\n\\n----------- Numerical text -----------\\n\\n{numerical_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee43b77",
   "metadata": {},
   "source": [
    "### 3. Training (10 Points)\n",
    "#### Train your RNN model on the prepared dataset. Aim to optimize the model to predict the next word in a sequence based on the given context. Adjust hyper- parameters such as learning rate, number of epochs, and hidden layer dimensions to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30f47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/552], Loss: 5.0324\n",
      "Epoch [1/100], Step [100/552], Loss: 5.0420\n",
      "Epoch [1/100], Step [200/552], Loss: 5.0411\n",
      "Epoch [1/100], Step [300/552], Loss: 4.9097\n",
      "Epoch [1/100], Step [400/552], Loss: 4.8458\n",
      "Epoch [1/100], Step [500/552], Loss: 4.8033\n",
      "Epoch [2/100], Step [0/552], Loss: 4.2868\n",
      "Epoch [2/100], Step [100/552], Loss: 4.8396\n",
      "Epoch [2/100], Step [200/552], Loss: 2.6307\n",
      "Epoch [2/100], Step [300/552], Loss: 4.2102\n",
      "Epoch [2/100], Step [400/552], Loss: 4.0916\n",
      "Epoch [2/100], Step [500/552], Loss: 3.7073\n",
      "Epoch [3/100], Step [0/552], Loss: 3.3117\n",
      "Epoch [3/100], Step [100/552], Loss: 4.5739\n",
      "Epoch [3/100], Step [200/552], Loss: 0.5607\n",
      "Epoch [3/100], Step [300/552], Loss: 3.1494\n",
      "Epoch [3/100], Step [400/552], Loss: 3.1376\n",
      "Epoch [3/100], Step [500/552], Loss: 2.6763\n",
      "Epoch [4/100], Step [0/552], Loss: 2.1112\n",
      "Epoch [4/100], Step [100/552], Loss: 4.1520\n",
      "Epoch [4/100], Step [200/552], Loss: 0.5199\n",
      "Epoch [4/100], Step [300/552], Loss: 1.7719\n",
      "Epoch [4/100], Step [400/552], Loss: 2.3927\n",
      "Epoch [4/100], Step [500/552], Loss: 2.1611\n",
      "Epoch [5/100], Step [0/552], Loss: 1.5250\n",
      "Epoch [5/100], Step [100/552], Loss: 3.6422\n",
      "Epoch [5/100], Step [200/552], Loss: 0.5213\n",
      "Epoch [5/100], Step [300/552], Loss: 0.9768\n",
      "Epoch [5/100], Step [400/552], Loss: 1.9339\n",
      "Epoch [5/100], Step [500/552], Loss: 1.7683\n",
      "Epoch [6/100], Step [0/552], Loss: 1.2090\n",
      "Epoch [6/100], Step [100/552], Loss: 3.1156\n",
      "Epoch [6/100], Step [200/552], Loss: 0.5136\n",
      "Epoch [6/100], Step [300/552], Loss: 0.7269\n",
      "Epoch [6/100], Step [400/552], Loss: 1.6887\n",
      "Epoch [6/100], Step [500/552], Loss: 1.5081\n",
      "Epoch [7/100], Step [0/552], Loss: 0.9665\n",
      "Epoch [7/100], Step [100/552], Loss: 2.6367\n",
      "Epoch [7/100], Step [200/552], Loss: 0.5119\n",
      "Epoch [7/100], Step [300/552], Loss: 0.6596\n",
      "Epoch [7/100], Step [400/552], Loss: 1.5093\n",
      "Epoch [7/100], Step [500/552], Loss: 1.3574\n",
      "Epoch [8/100], Step [0/552], Loss: 0.8187\n",
      "Epoch [8/100], Step [100/552], Loss: 2.2403\n",
      "Epoch [8/100], Step [200/552], Loss: 0.5080\n",
      "Epoch [8/100], Step [300/552], Loss: 0.6292\n",
      "Epoch [8/100], Step [400/552], Loss: 1.3827\n",
      "Epoch [8/100], Step [500/552], Loss: 1.2793\n",
      "Epoch [9/100], Step [0/552], Loss: 0.7517\n",
      "Epoch [9/100], Step [100/552], Loss: 1.9371\n",
      "Epoch [9/100], Step [200/552], Loss: 0.5095\n",
      "Epoch [9/100], Step [300/552], Loss: 0.6120\n",
      "Epoch [9/100], Step [400/552], Loss: 1.3082\n",
      "Epoch [9/100], Step [500/552], Loss: 1.2486\n",
      "Epoch [10/100], Step [0/552], Loss: 0.7237\n",
      "Epoch [10/100], Step [100/552], Loss: 1.7181\n",
      "Epoch [10/100], Step [200/552], Loss: 0.5006\n",
      "Epoch [10/100], Step [300/552], Loss: 0.6001\n",
      "Epoch [10/100], Step [400/552], Loss: 1.2721\n",
      "Epoch [10/100], Step [500/552], Loss: 1.2256\n",
      "Epoch [11/100], Step [0/552], Loss: 0.7117\n",
      "Epoch [11/100], Step [100/552], Loss: 1.5607\n",
      "Epoch [11/100], Step [200/552], Loss: 0.5733\n",
      "Epoch [11/100], Step [300/552], Loss: 0.5921\n",
      "Epoch [11/100], Step [400/552], Loss: 1.2545\n",
      "Epoch [11/100], Step [500/552], Loss: 1.2271\n",
      "Epoch [12/100], Step [0/552], Loss: 0.7136\n",
      "Epoch [12/100], Step [100/552], Loss: 1.4559\n",
      "Epoch [12/100], Step [200/552], Loss: 0.4950\n",
      "Epoch [12/100], Step [300/552], Loss: 0.5844\n",
      "Epoch [12/100], Step [400/552], Loss: 1.2436\n",
      "Epoch [12/100], Step [500/552], Loss: 1.2017\n",
      "Epoch [13/100], Step [0/552], Loss: 0.7699\n",
      "Epoch [13/100], Step [100/552], Loss: 1.3957\n",
      "Epoch [13/100], Step [200/552], Loss: 0.5343\n",
      "Epoch [13/100], Step [300/552], Loss: 0.5797\n",
      "Epoch [13/100], Step [400/552], Loss: 1.2356\n",
      "Epoch [13/100], Step [500/552], Loss: 1.2064\n",
      "Epoch [14/100], Step [0/552], Loss: 0.9140\n",
      "Epoch [14/100], Step [100/552], Loss: 1.3574\n",
      "Epoch [14/100], Step [200/552], Loss: 0.4911\n",
      "Epoch [14/100], Step [300/552], Loss: 0.5738\n",
      "Epoch [14/100], Step [400/552], Loss: 1.2294\n",
      "Epoch [14/100], Step [500/552], Loss: 1.1942\n",
      "Epoch [15/100], Step [0/552], Loss: 0.9838\n",
      "Epoch [15/100], Step [100/552], Loss: 1.3376\n",
      "Epoch [15/100], Step [200/552], Loss: 0.5091\n",
      "Epoch [15/100], Step [300/552], Loss: 0.5707\n",
      "Epoch [15/100], Step [400/552], Loss: 1.2226\n",
      "Epoch [15/100], Step [500/552], Loss: 1.1946\n",
      "Epoch [16/100], Step [0/552], Loss: 0.8564\n",
      "Epoch [16/100], Step [100/552], Loss: 1.3196\n",
      "Epoch [16/100], Step [200/552], Loss: 0.4913\n",
      "Epoch [16/100], Step [300/552], Loss: 0.5663\n",
      "Epoch [16/100], Step [400/552], Loss: 1.2181\n",
      "Epoch [16/100], Step [500/552], Loss: 1.1871\n",
      "Epoch [17/100], Step [0/552], Loss: 0.8193\n",
      "Epoch [17/100], Step [100/552], Loss: 1.3089\n",
      "Epoch [17/100], Step [200/552], Loss: 0.5039\n",
      "Epoch [17/100], Step [300/552], Loss: 0.5638\n",
      "Epoch [17/100], Step [400/552], Loss: 1.2130\n",
      "Epoch [17/100], Step [500/552], Loss: 1.1875\n",
      "Epoch [18/100], Step [0/552], Loss: 0.7625\n",
      "Epoch [18/100], Step [100/552], Loss: 1.2979\n",
      "Epoch [18/100], Step [200/552], Loss: 0.4898\n",
      "Epoch [18/100], Step [300/552], Loss: 0.5607\n",
      "Epoch [18/100], Step [400/552], Loss: 1.2093\n",
      "Epoch [18/100], Step [500/552], Loss: 1.1809\n",
      "Epoch [19/100], Step [0/552], Loss: 0.7538\n",
      "Epoch [19/100], Step [100/552], Loss: 1.2910\n",
      "Epoch [19/100], Step [200/552], Loss: 0.5040\n",
      "Epoch [19/100], Step [300/552], Loss: 0.5587\n",
      "Epoch [19/100], Step [400/552], Loss: 1.2052\n",
      "Epoch [19/100], Step [500/552], Loss: 1.1827\n",
      "Epoch [20/100], Step [0/552], Loss: 0.7359\n",
      "Epoch [20/100], Step [100/552], Loss: 1.2832\n",
      "Epoch [20/100], Step [200/552], Loss: 0.4872\n",
      "Epoch [20/100], Step [300/552], Loss: 0.5564\n",
      "Epoch [20/100], Step [400/552], Loss: 1.2021\n",
      "Epoch [20/100], Step [500/552], Loss: 1.1752\n",
      "Epoch [21/100], Step [0/552], Loss: 0.7325\n",
      "Epoch [21/100], Step [100/552], Loss: 1.2783\n",
      "Epoch [21/100], Step [200/552], Loss: 0.5100\n",
      "Epoch [21/100], Step [300/552], Loss: 0.5547\n",
      "Epoch [21/100], Step [400/552], Loss: 1.1986\n",
      "Epoch [21/100], Step [500/552], Loss: 1.1797\n",
      "Epoch [22/100], Step [0/552], Loss: 0.7203\n",
      "Epoch [22/100], Step [100/552], Loss: 1.2725\n",
      "Epoch [22/100], Step [200/552], Loss: 0.4849\n",
      "Epoch [22/100], Step [300/552], Loss: 0.5528\n",
      "Epoch [22/100], Step [400/552], Loss: 1.1959\n",
      "Epoch [22/100], Step [500/552], Loss: 1.1696\n",
      "Epoch [23/100], Step [0/552], Loss: 0.7212\n",
      "Epoch [23/100], Step [100/552], Loss: 1.2687\n",
      "Epoch [23/100], Step [200/552], Loss: 0.5191\n",
      "Epoch [23/100], Step [300/552], Loss: 0.5515\n",
      "Epoch [23/100], Step [400/552], Loss: 1.1930\n",
      "Epoch [23/100], Step [500/552], Loss: 1.1773\n",
      "Epoch [24/100], Step [0/552], Loss: 0.7140\n",
      "Epoch [24/100], Step [100/552], Loss: 1.2641\n",
      "Epoch [24/100], Step [200/552], Loss: 0.4833\n",
      "Epoch [24/100], Step [300/552], Loss: 0.5498\n",
      "Epoch [24/100], Step [400/552], Loss: 1.1905\n",
      "Epoch [24/100], Step [500/552], Loss: 1.1661\n",
      "Epoch [25/100], Step [0/552], Loss: 0.7168\n",
      "Epoch [25/100], Step [100/552], Loss: 1.2613\n",
      "Epoch [25/100], Step [200/552], Loss: 0.5145\n",
      "Epoch [25/100], Step [300/552], Loss: 0.5487\n",
      "Epoch [25/100], Step [400/552], Loss: 1.1880\n",
      "Epoch [25/100], Step [500/552], Loss: 1.1735\n",
      "Epoch [26/100], Step [0/552], Loss: 0.7112\n",
      "Epoch [26/100], Step [100/552], Loss: 1.2573\n",
      "Epoch [26/100], Step [200/552], Loss: 0.4828\n",
      "Epoch [26/100], Step [300/552], Loss: 0.5473\n",
      "Epoch [26/100], Step [400/552], Loss: 1.1858\n",
      "Epoch [26/100], Step [500/552], Loss: 1.1644\n",
      "Epoch [27/100], Step [0/552], Loss: 0.7153\n",
      "Epoch [27/100], Step [100/552], Loss: 1.2552\n",
      "Epoch [27/100], Step [200/552], Loss: 0.5053\n",
      "Epoch [27/100], Step [300/552], Loss: 0.5463\n",
      "Epoch [27/100], Step [400/552], Loss: 1.1835\n",
      "Epoch [27/100], Step [500/552], Loss: 1.1697\n",
      "Epoch [28/100], Step [0/552], Loss: 0.7101\n",
      "Epoch [28/100], Step [100/552], Loss: 1.2517\n",
      "Epoch [28/100], Step [200/552], Loss: 0.4832\n",
      "Epoch [28/100], Step [300/552], Loss: 0.5452\n",
      "Epoch [28/100], Step [400/552], Loss: 1.1817\n",
      "Epoch [28/100], Step [500/552], Loss: 1.1627\n",
      "Epoch [29/100], Step [0/552], Loss: 0.7148\n",
      "Epoch [29/100], Step [100/552], Loss: 1.2500\n",
      "Epoch [29/100], Step [200/552], Loss: 0.5002\n",
      "Epoch [29/100], Step [300/552], Loss: 0.5442\n",
      "Epoch [29/100], Step [400/552], Loss: 1.1795\n",
      "Epoch [29/100], Step [500/552], Loss: 1.1667\n",
      "Epoch [30/100], Step [0/552], Loss: 0.7091\n",
      "Epoch [30/100], Step [100/552], Loss: 1.2469\n",
      "Epoch [30/100], Step [200/552], Loss: 0.4833\n",
      "Epoch [30/100], Step [300/552], Loss: 0.5433\n",
      "Epoch [30/100], Step [400/552], Loss: 1.1779\n",
      "Epoch [30/100], Step [500/552], Loss: 1.1609\n",
      "Epoch [31/100], Step [0/552], Loss: 0.7139\n",
      "Epoch [31/100], Step [100/552], Loss: 1.2456\n",
      "Epoch [31/100], Step [200/552], Loss: 0.4980\n",
      "Epoch [31/100], Step [300/552], Loss: 0.5424\n",
      "Epoch [31/100], Step [400/552], Loss: 1.1759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [500/552], Loss: 1.1645\n",
      "Epoch [32/100], Step [0/552], Loss: 0.7074\n",
      "Epoch [32/100], Step [100/552], Loss: 1.2429\n",
      "Epoch [32/100], Step [200/552], Loss: 0.4828\n",
      "Epoch [32/100], Step [300/552], Loss: 0.5416\n",
      "Epoch [32/100], Step [400/552], Loss: 1.1745\n",
      "Epoch [32/100], Step [500/552], Loss: 1.1588\n",
      "Epoch [33/100], Step [0/552], Loss: 0.7121\n",
      "Epoch [33/100], Step [100/552], Loss: 1.2418\n",
      "Epoch [33/100], Step [200/552], Loss: 0.4978\n",
      "Epoch [33/100], Step [300/552], Loss: 0.5408\n",
      "Epoch [33/100], Step [400/552], Loss: 1.1727\n",
      "Epoch [33/100], Step [500/552], Loss: 1.1627\n",
      "Epoch [34/100], Step [0/552], Loss: 0.7049\n",
      "Epoch [34/100], Step [100/552], Loss: 1.2393\n",
      "Epoch [34/100], Step [200/552], Loss: 0.4818\n",
      "Epoch [34/100], Step [300/552], Loss: 0.5400\n",
      "Epoch [34/100], Step [400/552], Loss: 1.1714\n",
      "Epoch [34/100], Step [500/552], Loss: 1.1567\n",
      "Epoch [35/100], Step [0/552], Loss: 0.7095\n",
      "Epoch [35/100], Step [100/552], Loss: 1.2384\n",
      "Epoch [35/100], Step [200/552], Loss: 0.4991\n",
      "Epoch [35/100], Step [300/552], Loss: 0.5393\n",
      "Epoch [35/100], Step [400/552], Loss: 1.1697\n",
      "Epoch [35/100], Step [500/552], Loss: 1.1614\n",
      "Epoch [36/100], Step [0/552], Loss: 0.7020\n",
      "Epoch [36/100], Step [100/552], Loss: 1.2361\n",
      "Epoch [36/100], Step [200/552], Loss: 0.4807\n",
      "Epoch [36/100], Step [300/552], Loss: 0.5386\n",
      "Epoch [36/100], Step [400/552], Loss: 1.1685\n",
      "Epoch [36/100], Step [500/552], Loss: 1.1545\n",
      "Epoch [37/100], Step [0/552], Loss: 0.7065\n",
      "Epoch [37/100], Step [100/552], Loss: 1.2353\n",
      "Epoch [37/100], Step [200/552], Loss: 0.5011\n",
      "Epoch [37/100], Step [300/552], Loss: 0.5380\n",
      "Epoch [37/100], Step [400/552], Loss: 1.1669\n",
      "Epoch [37/100], Step [500/552], Loss: 1.1602\n",
      "Epoch [38/100], Step [0/552], Loss: 0.6990\n",
      "Epoch [38/100], Step [100/552], Loss: 1.2333\n",
      "Epoch [38/100], Step [200/552], Loss: 0.4799\n",
      "Epoch [38/100], Step [300/552], Loss: 0.5374\n",
      "Epoch [38/100], Step [400/552], Loss: 1.1658\n",
      "Epoch [38/100], Step [500/552], Loss: 1.1526\n",
      "Epoch [39/100], Step [0/552], Loss: 0.7036\n",
      "Epoch [39/100], Step [100/552], Loss: 1.2326\n",
      "Epoch [39/100], Step [200/552], Loss: 0.5019\n",
      "Epoch [39/100], Step [300/552], Loss: 0.5368\n",
      "Epoch [39/100], Step [400/552], Loss: 1.1644\n",
      "Epoch [39/100], Step [500/552], Loss: 1.1589\n",
      "Epoch [40/100], Step [0/552], Loss: 0.6962\n",
      "Epoch [40/100], Step [100/552], Loss: 1.2307\n",
      "Epoch [40/100], Step [200/552], Loss: 0.4794\n",
      "Epoch [40/100], Step [300/552], Loss: 0.5362\n",
      "Epoch [40/100], Step [400/552], Loss: 1.1633\n",
      "Epoch [40/100], Step [500/552], Loss: 1.1512\n",
      "Epoch [41/100], Step [0/552], Loss: 0.7011\n",
      "Epoch [41/100], Step [100/552], Loss: 1.2301\n",
      "Epoch [41/100], Step [200/552], Loss: 0.5004\n",
      "Epoch [41/100], Step [300/552], Loss: 0.5357\n",
      "Epoch [41/100], Step [400/552], Loss: 1.1620\n",
      "Epoch [41/100], Step [500/552], Loss: 1.1573\n",
      "Epoch [42/100], Step [0/552], Loss: 0.6939\n",
      "Epoch [42/100], Step [100/552], Loss: 1.2284\n",
      "Epoch [42/100], Step [200/552], Loss: 0.4793\n",
      "Epoch [42/100], Step [300/552], Loss: 0.5352\n",
      "Epoch [42/100], Step [400/552], Loss: 1.1611\n",
      "Epoch [42/100], Step [500/552], Loss: 1.1502\n",
      "Epoch [43/100], Step [0/552], Loss: 0.6990\n",
      "Epoch [43/100], Step [100/552], Loss: 1.2278\n",
      "Epoch [43/100], Step [200/552], Loss: 0.4977\n",
      "Epoch [43/100], Step [300/552], Loss: 0.5347\n",
      "Epoch [43/100], Step [400/552], Loss: 1.1598\n",
      "Epoch [43/100], Step [500/552], Loss: 1.1556\n",
      "Epoch [44/100], Step [0/552], Loss: 0.6920\n",
      "Epoch [44/100], Step [100/552], Loss: 1.2262\n",
      "Epoch [44/100], Step [200/552], Loss: 0.4793\n",
      "Epoch [44/100], Step [300/552], Loss: 0.5342\n",
      "Epoch [44/100], Step [400/552], Loss: 1.1589\n",
      "Epoch [44/100], Step [500/552], Loss: 1.1494\n",
      "Epoch [45/100], Step [0/552], Loss: 0.6974\n",
      "Epoch [45/100], Step [100/552], Loss: 1.2257\n",
      "Epoch [45/100], Step [200/552], Loss: 0.4954\n",
      "Epoch [45/100], Step [300/552], Loss: 0.5337\n",
      "Epoch [45/100], Step [400/552], Loss: 1.1577\n",
      "Epoch [45/100], Step [500/552], Loss: 1.1541\n",
      "Epoch [46/100], Step [0/552], Loss: 0.6905\n",
      "Epoch [46/100], Step [100/552], Loss: 1.2242\n",
      "Epoch [46/100], Step [200/552], Loss: 0.4794\n",
      "Epoch [46/100], Step [300/552], Loss: 0.5333\n",
      "Epoch [46/100], Step [400/552], Loss: 1.1569\n",
      "Epoch [46/100], Step [500/552], Loss: 1.1484\n",
      "Epoch [47/100], Step [0/552], Loss: 0.6961\n",
      "Epoch [47/100], Step [100/552], Loss: 1.2238\n",
      "Epoch [47/100], Step [200/552], Loss: 0.4939\n",
      "Epoch [47/100], Step [300/552], Loss: 0.5328\n",
      "Epoch [47/100], Step [400/552], Loss: 1.1558\n",
      "Epoch [47/100], Step [500/552], Loss: 1.1528\n",
      "Epoch [48/100], Step [0/552], Loss: 0.6892\n",
      "Epoch [48/100], Step [100/552], Loss: 1.2224\n",
      "Epoch [48/100], Step [200/552], Loss: 0.4793\n",
      "Epoch [48/100], Step [300/552], Loss: 0.5324\n",
      "Epoch [48/100], Step [400/552], Loss: 1.1550\n",
      "Epoch [48/100], Step [500/552], Loss: 1.1474\n",
      "Epoch [49/100], Step [0/552], Loss: 0.6951\n",
      "Epoch [49/100], Step [100/552], Loss: 1.2220\n",
      "Epoch [49/100], Step [200/552], Loss: 0.4933\n",
      "Epoch [49/100], Step [300/552], Loss: 0.5320\n",
      "Epoch [49/100], Step [400/552], Loss: 1.1539\n",
      "Epoch [49/100], Step [500/552], Loss: 1.1517\n",
      "Epoch [50/100], Step [0/552], Loss: 0.6881\n",
      "Epoch [50/100], Step [100/552], Loss: 1.2207\n",
      "Epoch [50/100], Step [200/552], Loss: 0.4789\n",
      "Epoch [50/100], Step [300/552], Loss: 0.5317\n",
      "Epoch [50/100], Step [400/552], Loss: 1.1532\n",
      "Epoch [50/100], Step [500/552], Loss: 1.1463\n",
      "Epoch [51/100], Step [0/552], Loss: 0.6941\n",
      "Epoch [51/100], Step [100/552], Loss: 1.2203\n",
      "Epoch [51/100], Step [200/552], Loss: 0.4932\n",
      "Epoch [51/100], Step [300/552], Loss: 0.5313\n",
      "Epoch [51/100], Step [400/552], Loss: 1.1522\n",
      "Epoch [51/100], Step [500/552], Loss: 1.1507\n",
      "Epoch [52/100], Step [0/552], Loss: 0.6870\n",
      "Epoch [52/100], Step [100/552], Loss: 1.2191\n",
      "Epoch [52/100], Step [200/552], Loss: 0.4784\n",
      "Epoch [52/100], Step [300/552], Loss: 0.5309\n",
      "Epoch [52/100], Step [400/552], Loss: 1.1515\n",
      "Epoch [52/100], Step [500/552], Loss: 1.1452\n",
      "Epoch [53/100], Step [0/552], Loss: 0.6932\n",
      "Epoch [53/100], Step [100/552], Loss: 1.2187\n",
      "Epoch [53/100], Step [200/552], Loss: 0.4936\n",
      "Epoch [53/100], Step [300/552], Loss: 0.5305\n",
      "Epoch [53/100], Step [400/552], Loss: 1.1506\n",
      "Epoch [53/100], Step [500/552], Loss: 1.1499\n",
      "Epoch [54/100], Step [0/552], Loss: 0.6860\n",
      "Epoch [54/100], Step [100/552], Loss: 1.2176\n",
      "Epoch [54/100], Step [200/552], Loss: 0.4779\n",
      "Epoch [54/100], Step [300/552], Loss: 0.5302\n",
      "Epoch [54/100], Step [400/552], Loss: 1.1498\n",
      "Epoch [54/100], Step [500/552], Loss: 1.1441\n",
      "Epoch [55/100], Step [0/552], Loss: 0.6922\n",
      "Epoch [55/100], Step [100/552], Loss: 1.2173\n",
      "Epoch [55/100], Step [200/552], Loss: 0.4941\n",
      "Epoch [55/100], Step [300/552], Loss: 0.5299\n",
      "Epoch [55/100], Step [400/552], Loss: 1.1490\n",
      "Epoch [55/100], Step [500/552], Loss: 1.1491\n",
      "Epoch [56/100], Step [0/552], Loss: 0.6849\n",
      "Epoch [56/100], Step [100/552], Loss: 1.2162\n",
      "Epoch [56/100], Step [200/552], Loss: 0.4775\n",
      "Epoch [56/100], Step [300/552], Loss: 0.5295\n",
      "Epoch [56/100], Step [400/552], Loss: 1.1483\n",
      "Epoch [56/100], Step [500/552], Loss: 1.1430\n",
      "Epoch [57/100], Step [0/552], Loss: 0.6912\n",
      "Epoch [57/100], Step [100/552], Loss: 1.2158\n",
      "Epoch [57/100], Step [200/552], Loss: 0.4942\n",
      "Epoch [57/100], Step [300/552], Loss: 0.5292\n",
      "Epoch [57/100], Step [400/552], Loss: 1.1475\n",
      "Epoch [57/100], Step [500/552], Loss: 1.1483\n",
      "Epoch [58/100], Step [0/552], Loss: 0.6838\n",
      "Epoch [58/100], Step [100/552], Loss: 1.2148\n",
      "Epoch [58/100], Step [200/552], Loss: 0.4772\n",
      "Epoch [58/100], Step [300/552], Loss: 0.5289\n",
      "Epoch [58/100], Step [400/552], Loss: 1.1468\n",
      "Epoch [58/100], Step [500/552], Loss: 1.1421\n",
      "Epoch [59/100], Step [0/552], Loss: 0.6902\n",
      "Epoch [59/100], Step [100/552], Loss: 1.2145\n",
      "Epoch [59/100], Step [200/552], Loss: 0.4937\n",
      "Epoch [59/100], Step [300/552], Loss: 0.5286\n",
      "Epoch [59/100], Step [400/552], Loss: 1.1461\n",
      "Epoch [59/100], Step [500/552], Loss: 1.1474\n",
      "Epoch [60/100], Step [0/552], Loss: 0.6827\n",
      "Epoch [60/100], Step [100/552], Loss: 1.2136\n",
      "Epoch [60/100], Step [200/552], Loss: 0.4771\n",
      "Epoch [60/100], Step [300/552], Loss: 0.5283\n",
      "Epoch [60/100], Step [400/552], Loss: 1.1454\n",
      "Epoch [60/100], Step [500/552], Loss: 1.1414\n",
      "Epoch [61/100], Step [0/552], Loss: 0.6892\n",
      "Epoch [61/100], Step [100/552], Loss: 1.2133\n",
      "Epoch [61/100], Step [200/552], Loss: 0.4927\n",
      "Epoch [61/100], Step [300/552], Loss: 0.5280\n",
      "Epoch [61/100], Step [400/552], Loss: 1.1447\n",
      "Epoch [61/100], Step [500/552], Loss: 1.1464\n",
      "Epoch [62/100], Step [0/552], Loss: 0.6817\n",
      "Epoch [62/100], Step [100/552], Loss: 1.2124\n",
      "Epoch [62/100], Step [200/552], Loss: 0.4771\n",
      "Epoch [62/100], Step [300/552], Loss: 0.5278\n",
      "Epoch [62/100], Step [400/552], Loss: 1.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Step [500/552], Loss: 1.1408\n",
      "Epoch [63/100], Step [0/552], Loss: 0.6883\n",
      "Epoch [63/100], Step [100/552], Loss: 1.2121\n",
      "Epoch [63/100], Step [200/552], Loss: 0.4916\n",
      "Epoch [63/100], Step [300/552], Loss: 0.5274\n",
      "Epoch [63/100], Step [400/552], Loss: 1.1433\n",
      "Epoch [63/100], Step [500/552], Loss: 1.1454\n",
      "Epoch [64/100], Step [0/552], Loss: 0.6807\n",
      "Epoch [64/100], Step [100/552], Loss: 1.2112\n",
      "Epoch [64/100], Step [200/552], Loss: 0.4771\n",
      "Epoch [64/100], Step [300/552], Loss: 0.5272\n",
      "Epoch [64/100], Step [400/552], Loss: 1.1428\n",
      "Epoch [64/100], Step [500/552], Loss: 1.1401\n",
      "Epoch [65/100], Step [0/552], Loss: 0.6874\n",
      "Epoch [65/100], Step [100/552], Loss: 1.2110\n",
      "Epoch [65/100], Step [200/552], Loss: 0.4907\n",
      "Epoch [65/100], Step [300/552], Loss: 0.5269\n",
      "Epoch [65/100], Step [400/552], Loss: 1.1421\n",
      "Epoch [65/100], Step [500/552], Loss: 1.1445\n",
      "Epoch [66/100], Step [0/552], Loss: 0.6798\n",
      "Epoch [66/100], Step [100/552], Loss: 1.2101\n",
      "Epoch [66/100], Step [200/552], Loss: 0.4771\n",
      "Epoch [66/100], Step [300/552], Loss: 0.5267\n",
      "Epoch [66/100], Step [400/552], Loss: 1.1415\n",
      "Epoch [66/100], Step [500/552], Loss: 1.1394\n",
      "Epoch [67/100], Step [0/552], Loss: 0.6866\n",
      "Epoch [67/100], Step [100/552], Loss: 1.2099\n",
      "Epoch [67/100], Step [200/552], Loss: 0.4901\n",
      "Epoch [67/100], Step [300/552], Loss: 0.5264\n",
      "Epoch [67/100], Step [400/552], Loss: 1.1409\n",
      "Epoch [67/100], Step [500/552], Loss: 1.1437\n",
      "Epoch [68/100], Step [0/552], Loss: 0.6790\n",
      "Epoch [68/100], Step [100/552], Loss: 1.2091\n",
      "Epoch [68/100], Step [200/552], Loss: 0.4769\n",
      "Epoch [68/100], Step [300/552], Loss: 0.5262\n",
      "Epoch [68/100], Step [400/552], Loss: 1.1403\n",
      "Epoch [68/100], Step [500/552], Loss: 1.1387\n",
      "Epoch [69/100], Step [0/552], Loss: 0.6859\n",
      "Epoch [69/100], Step [100/552], Loss: 1.2088\n",
      "Epoch [69/100], Step [200/552], Loss: 0.4898\n",
      "Epoch [69/100], Step [300/552], Loss: 0.5259\n",
      "Epoch [69/100], Step [400/552], Loss: 1.1397\n",
      "Epoch [69/100], Step [500/552], Loss: 1.1430\n",
      "Epoch [70/100], Step [0/552], Loss: 0.6783\n",
      "Epoch [70/100], Step [100/552], Loss: 1.2081\n",
      "Epoch [70/100], Step [200/552], Loss: 0.4767\n",
      "Epoch [70/100], Step [300/552], Loss: 0.5257\n",
      "Epoch [70/100], Step [400/552], Loss: 1.1392\n",
      "Epoch [70/100], Step [500/552], Loss: 1.1380\n",
      "Epoch [71/100], Step [0/552], Loss: 0.6853\n",
      "Epoch [71/100], Step [100/552], Loss: 1.2079\n",
      "Epoch [71/100], Step [200/552], Loss: 0.4897\n",
      "Epoch [71/100], Step [300/552], Loss: 0.5254\n",
      "Epoch [71/100], Step [400/552], Loss: 1.1386\n",
      "Epoch [71/100], Step [500/552], Loss: 1.1424\n",
      "Epoch [72/100], Step [0/552], Loss: 0.6776\n",
      "Epoch [72/100], Step [100/552], Loss: 1.2071\n",
      "Epoch [72/100], Step [200/552], Loss: 0.4765\n",
      "Epoch [72/100], Step [300/552], Loss: 0.5253\n",
      "Epoch [72/100], Step [400/552], Loss: 1.1380\n",
      "Epoch [72/100], Step [500/552], Loss: 1.1372\n",
      "Epoch [73/100], Step [0/552], Loss: 0.6846\n",
      "Epoch [73/100], Step [100/552], Loss: 1.2069\n",
      "Epoch [73/100], Step [200/552], Loss: 0.4898\n",
      "Epoch [73/100], Step [300/552], Loss: 0.5250\n",
      "Epoch [73/100], Step [400/552], Loss: 1.1376\n",
      "Epoch [73/100], Step [500/552], Loss: 1.1418\n",
      "Epoch [74/100], Step [0/552], Loss: 0.6769\n",
      "Epoch [74/100], Step [100/552], Loss: 1.2062\n",
      "Epoch [74/100], Step [200/552], Loss: 0.4762\n",
      "Epoch [74/100], Step [300/552], Loss: 0.5248\n",
      "Epoch [74/100], Step [400/552], Loss: 1.1368\n",
      "Epoch [74/100], Step [500/552], Loss: 1.1364\n",
      "Epoch [75/100], Step [0/552], Loss: 0.6840\n",
      "Epoch [75/100], Step [100/552], Loss: 1.2060\n",
      "Epoch [75/100], Step [200/552], Loss: 0.4899\n",
      "Epoch [75/100], Step [300/552], Loss: 0.5245\n",
      "Epoch [75/100], Step [400/552], Loss: 1.1367\n",
      "Epoch [75/100], Step [500/552], Loss: 1.1413\n",
      "Epoch [76/100], Step [0/552], Loss: 0.6762\n",
      "Epoch [76/100], Step [100/552], Loss: 1.2053\n",
      "Epoch [76/100], Step [200/552], Loss: 0.4760\n",
      "Epoch [76/100], Step [300/552], Loss: 0.5244\n",
      "Epoch [76/100], Step [400/552], Loss: 1.1357\n",
      "Epoch [76/100], Step [500/552], Loss: 1.1356\n",
      "Epoch [77/100], Step [0/552], Loss: 0.6834\n",
      "Epoch [77/100], Step [100/552], Loss: 1.2051\n",
      "Epoch [77/100], Step [200/552], Loss: 0.4898\n",
      "Epoch [77/100], Step [300/552], Loss: 0.5241\n",
      "Epoch [77/100], Step [400/552], Loss: 1.1357\n",
      "Epoch [77/100], Step [500/552], Loss: 1.1407\n",
      "Epoch [78/100], Step [0/552], Loss: 0.6755\n",
      "Epoch [78/100], Step [100/552], Loss: 1.2044\n",
      "Epoch [78/100], Step [200/552], Loss: 0.4759\n",
      "Epoch [78/100], Step [300/552], Loss: 0.5240\n",
      "Epoch [78/100], Step [400/552], Loss: 1.1348\n",
      "Epoch [78/100], Step [500/552], Loss: 1.1350\n",
      "Epoch [79/100], Step [0/552], Loss: 0.6828\n",
      "Epoch [79/100], Step [100/552], Loss: 1.2042\n",
      "Epoch [79/100], Step [200/552], Loss: 0.4895\n",
      "Epoch [79/100], Step [300/552], Loss: 0.5237\n",
      "Epoch [79/100], Step [400/552], Loss: 1.1346\n",
      "Epoch [79/100], Step [500/552], Loss: 1.1399\n",
      "Epoch [80/100], Step [0/552], Loss: 0.6748\n",
      "Epoch [80/100], Step [100/552], Loss: 1.2036\n",
      "Epoch [80/100], Step [200/552], Loss: 0.4758\n",
      "Epoch [80/100], Step [300/552], Loss: 0.5236\n",
      "Epoch [80/100], Step [400/552], Loss: 1.1339\n",
      "Epoch [80/100], Step [500/552], Loss: 1.1345\n",
      "Epoch [81/100], Step [0/552], Loss: 0.6822\n",
      "Epoch [81/100], Step [100/552], Loss: 1.2034\n",
      "Epoch [81/100], Step [200/552], Loss: 0.4890\n",
      "Epoch [81/100], Step [300/552], Loss: 0.5233\n",
      "Epoch [81/100], Step [400/552], Loss: 1.1336\n",
      "Epoch [81/100], Step [500/552], Loss: 1.1392\n",
      "Epoch [82/100], Step [0/552], Loss: 0.6742\n",
      "Epoch [82/100], Step [100/552], Loss: 1.2028\n",
      "Epoch [82/100], Step [200/552], Loss: 0.4757\n",
      "Epoch [82/100], Step [300/552], Loss: 0.5232\n",
      "Epoch [82/100], Step [400/552], Loss: 1.1330\n",
      "Epoch [82/100], Step [500/552], Loss: 1.1340\n",
      "Epoch [83/100], Step [0/552], Loss: 0.6816\n",
      "Epoch [83/100], Step [100/552], Loss: 1.2026\n",
      "Epoch [83/100], Step [200/552], Loss: 0.4885\n",
      "Epoch [83/100], Step [300/552], Loss: 0.5229\n",
      "Epoch [83/100], Step [400/552], Loss: 1.1326\n",
      "Epoch [83/100], Step [500/552], Loss: 1.1385\n",
      "Epoch [84/100], Step [0/552], Loss: 0.6736\n",
      "Epoch [84/100], Step [100/552], Loss: 1.2020\n",
      "Epoch [84/100], Step [200/552], Loss: 0.4757\n",
      "Epoch [84/100], Step [300/552], Loss: 0.5228\n",
      "Epoch [84/100], Step [400/552], Loss: 1.1321\n",
      "Epoch [84/100], Step [500/552], Loss: 1.1335\n",
      "Epoch [85/100], Step [0/552], Loss: 0.6811\n",
      "Epoch [85/100], Step [100/552], Loss: 1.2018\n",
      "Epoch [85/100], Step [200/552], Loss: 0.4880\n",
      "Epoch [85/100], Step [300/552], Loss: 0.5225\n",
      "Epoch [85/100], Step [400/552], Loss: 1.1318\n",
      "Epoch [85/100], Step [500/552], Loss: 1.1379\n",
      "Epoch [86/100], Step [0/552], Loss: 0.6730\n",
      "Epoch [86/100], Step [100/552], Loss: 1.2012\n",
      "Epoch [86/100], Step [200/552], Loss: 0.4756\n",
      "Epoch [86/100], Step [300/552], Loss: 0.5224\n",
      "Epoch [86/100], Step [400/552], Loss: 1.1312\n",
      "Epoch [86/100], Step [500/552], Loss: 1.1330\n",
      "Epoch [87/100], Step [0/552], Loss: 0.6805\n",
      "Epoch [87/100], Step [100/552], Loss: 1.2011\n",
      "Epoch [87/100], Step [200/552], Loss: 0.4876\n",
      "Epoch [87/100], Step [300/552], Loss: 0.5222\n",
      "Epoch [87/100], Step [400/552], Loss: 1.1309\n",
      "Epoch [87/100], Step [500/552], Loss: 1.1372\n",
      "Epoch [88/100], Step [0/552], Loss: 0.6724\n",
      "Epoch [88/100], Step [100/552], Loss: 1.2005\n",
      "Epoch [88/100], Step [200/552], Loss: 0.4756\n",
      "Epoch [88/100], Step [300/552], Loss: 0.5221\n",
      "Epoch [88/100], Step [400/552], Loss: 1.1303\n",
      "Epoch [88/100], Step [500/552], Loss: 1.1325\n",
      "Epoch [89/100], Step [0/552], Loss: 0.6800\n",
      "Epoch [89/100], Step [100/552], Loss: 1.2003\n",
      "Epoch [89/100], Step [200/552], Loss: 0.4873\n",
      "Epoch [89/100], Step [300/552], Loss: 0.5218\n",
      "Epoch [89/100], Step [400/552], Loss: 1.1301\n",
      "Epoch [89/100], Step [500/552], Loss: 1.1367\n",
      "Epoch [90/100], Step [0/552], Loss: 0.6719\n",
      "Epoch [90/100], Step [100/552], Loss: 1.1998\n",
      "Epoch [90/100], Step [200/552], Loss: 0.4755\n",
      "Epoch [90/100], Step [300/552], Loss: 0.5217\n",
      "Epoch [90/100], Step [400/552], Loss: 1.1295\n",
      "Epoch [90/100], Step [500/552], Loss: 1.1319\n",
      "Epoch [91/100], Step [0/552], Loss: 0.6795\n",
      "Epoch [91/100], Step [100/552], Loss: 1.1996\n",
      "Epoch [91/100], Step [200/552], Loss: 0.4871\n",
      "Epoch [91/100], Step [300/552], Loss: 0.5215\n",
      "Epoch [91/100], Step [400/552], Loss: 1.1293\n",
      "Epoch [91/100], Step [500/552], Loss: 1.1362\n",
      "Epoch [92/100], Step [0/552], Loss: 0.6713\n",
      "Epoch [92/100], Step [100/552], Loss: 1.1991\n",
      "Epoch [92/100], Step [200/552], Loss: 0.4754\n",
      "Epoch [92/100], Step [300/552], Loss: 0.5214\n",
      "Epoch [92/100], Step [400/552], Loss: 1.1286\n",
      "Epoch [92/100], Step [500/552], Loss: 1.1314\n",
      "Epoch [93/100], Step [0/552], Loss: 0.6790\n",
      "Epoch [93/100], Step [100/552], Loss: 1.1990\n",
      "Epoch [93/100], Step [200/552], Loss: 0.4870\n",
      "Epoch [93/100], Step [300/552], Loss: 0.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Step [400/552], Loss: 1.1285\n",
      "Epoch [93/100], Step [500/552], Loss: 1.1357\n",
      "Epoch [94/100], Step [0/552], Loss: 0.6708\n",
      "Epoch [94/100], Step [100/552], Loss: 1.1984\n",
      "Epoch [94/100], Step [200/552], Loss: 0.4752\n",
      "Epoch [94/100], Step [300/552], Loss: 0.5210\n",
      "Epoch [94/100], Step [400/552], Loss: 1.1278\n",
      "Epoch [94/100], Step [500/552], Loss: 1.1308\n",
      "Epoch [95/100], Step [0/552], Loss: 0.6786\n",
      "Epoch [95/100], Step [100/552], Loss: 1.1983\n",
      "Epoch [95/100], Step [200/552], Loss: 0.4869\n",
      "Epoch [95/100], Step [300/552], Loss: 0.5208\n",
      "Epoch [95/100], Step [400/552], Loss: 1.1277\n",
      "Epoch [95/100], Step [500/552], Loss: 1.1352\n",
      "Epoch [96/100], Step [0/552], Loss: 0.6703\n",
      "Epoch [96/100], Step [100/552], Loss: 1.1977\n",
      "Epoch [96/100], Step [200/552], Loss: 0.4750\n",
      "Epoch [96/100], Step [300/552], Loss: 0.5207\n",
      "Epoch [96/100], Step [400/552], Loss: 1.1271\n",
      "Epoch [96/100], Step [500/552], Loss: 1.1303\n",
      "Epoch [97/100], Step [0/552], Loss: 0.6781\n",
      "Epoch [97/100], Step [100/552], Loss: 1.1976\n",
      "Epoch [97/100], Step [200/552], Loss: 0.4869\n",
      "Epoch [97/100], Step [300/552], Loss: 0.5204\n",
      "Epoch [97/100], Step [400/552], Loss: 1.1269\n",
      "Epoch [97/100], Step [500/552], Loss: 1.1347\n",
      "Epoch [98/100], Step [0/552], Loss: 0.6698\n",
      "Epoch [98/100], Step [100/552], Loss: 1.1971\n",
      "Epoch [98/100], Step [200/552], Loss: 0.4749\n",
      "Epoch [98/100], Step [300/552], Loss: 0.5204\n",
      "Epoch [98/100], Step [400/552], Loss: 1.1263\n",
      "Epoch [98/100], Step [500/552], Loss: 1.1298\n",
      "Epoch [99/100], Step [0/552], Loss: 0.6777\n",
      "Epoch [99/100], Step [100/552], Loss: 1.1970\n",
      "Epoch [99/100], Step [200/552], Loss: 0.4867\n",
      "Epoch [99/100], Step [300/552], Loss: 0.5201\n",
      "Epoch [99/100], Step [400/552], Loss: 1.1261\n",
      "Epoch [99/100], Step [500/552], Loss: 1.1342\n",
      "Epoch [100/100], Step [0/552], Loss: 0.6694\n",
      "Epoch [100/100], Step [100/552], Loss: 1.1964\n",
      "Epoch [100/100], Step [200/552], Loss: 0.4748\n",
      "Epoch [100/100], Step [300/552], Loss: 0.5200\n",
      "Epoch [100/100], Step [400/552], Loss: 1.1256\n",
      "Epoch [100/100], Step [500/552], Loss: 1.1294\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "seq_length = 10  # Sequence length for training\n",
    "\n",
    "# Convert numerical text to PyTorch tensor\n",
    "numerical_text_tensor = torch.tensor(numerical_text)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(numerical_text_tensor) - seq_length, seq_length):\n",
    "        # Get input and target sequences\n",
    "        inputs = numerical_text_tensor[i:i+seq_length].unsqueeze(0)\n",
    "        targets = numerical_text_tensor[i+1:i+1+seq_length].view(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(numerical_text_tensor)-seq_length}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bf015",
   "metadata": {},
   "source": [
    "### 4. Text Generation\n",
    "#### Once trained, use your model to generate text. Start with a seed sentence or word, then predict the next word using your model. Append the predicted word to your text and use the updated sequence as the new input to generate the next word. Repeat this process to generate a text of at least 100 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df75367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Generated Text ------------\n",
      "\n",
      "You can stand under my umbrella you need and we ll mend your friend took an oath i ma stick it s raining you can stand under my arms it s no clouds come into me you need and more because when the world has dealt its cards if the world has dealt its cards if the rain come in the take three action uh huh uh huh uh huh war has dealt its cards if the bank comin down like the rain come into me there s no distance in between you can stand under my umbrella you need and more because when the end\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('rnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, start_text, length=100):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # Tokenize the current text\n",
    "            tokens = preprocess_text(current_text)\n",
    "            numerical_tokens = numerify_text(tokens, vocab)\n",
    "\n",
    "            # Convert to PyTorch tensor and add batch dimension\n",
    "            input_tensor = torch.tensor(numerical_tokens).unsqueeze(0)\n",
    "\n",
    "            # Forward pass to predict the next word\n",
    "            output = model(input_tensor)\n",
    "\n",
    "            # Reshape output to (batch_size, sequence_length, vocab_size)\n",
    "            output = output.view(1, -1, output_dim)\n",
    "\n",
    "            # Apply softmax along the vocab dimension\n",
    "            probabilities = F.softmax(output, dim=2).squeeze().detach().numpy()\n",
    "\n",
    "            # Sample the next word based on the predicted probabilities\n",
    "            predicted_index = np.random.choice(len(probabilities[-1]), p=probabilities[-1])\n",
    "            predicted_word = list(vocab.keys())[list(vocab.values()).index(predicted_index)]\n",
    "\n",
    "            # Append the predicted word to the current text\n",
    "            current_text += ' ' + predicted_word\n",
    "\n",
    "    return current_text\n",
    "\n",
    "# Seed sentence to start text generation\n",
    "seed_sentence = \"You can stand under my umbrella\"\n",
    "\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(model, seed_sentence, length=100)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"------------ Generated Text ------------\\n\\n{generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd202e4",
   "metadata": {},
   "source": [
    "### 5. Analysis\n",
    "#### Analyze the generated text. Discuss how well your model captures the style and coherence of the chosen dataset. Reflect on the performance of the basic RNN model versus the LSTM-enhanced version. Consider the effects of different hyper- parameters on the quality of the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b03753",
   "metadata": {},
   "source": [
    "##### Style and Coherence Analysis:\n",
    "The generated text continues to include phrases and words from the original song \"Umbrella\" by Rihanna, such as \"under my umbrella,\" \"ella eh eh,\" and \"stand under my umbrella.\" This indicates that the model has retained the stylistic elements of the song.\n",
    "\n",
    "However, similar to the previous generated text, there are instances where the coherence is compromised. For example, phrases like \"have each other you can stand under my umbrella\" and \"under my entity here for the dow jones\" do not make sense in the context of the song lyrics. This suggests that while the model has learned some patterns from the training data, it may struggle with maintaining coherence and understanding context over longer sequences.\n",
    "\n",
    "##### Comparison with Original Song:\n",
    "The generated text still includes elements from the original song, such as the repeated phrase \"under my umbrella ella eh eh\" and the mention of standing under an umbrella. However, it also introduces new phrases and words that are not present in the original song lyrics.\n",
    "\n",
    "##### Reflecting on Model Performance:\n",
    "Basic RNN vs. LSTM-Enhanced Model: It's likely that the LSTM-enhanced model performed better in capturing the long-term dependencies and stylistic nuances of the song lyrics compared to the basic RNN. However, even with LSTM, the generated text still exhibits some inconsistencies and lacks full coherence.\n",
    "Effects of Hyperparameters:\n",
    "Adjusting hyperparameters such as the learning rate, number of epochs, and hidden layer dimensions may have an impact on the quality of the generated text. Fine-tuning these hyperparameters could potentially lead to better results.\n",
    "\n",
    "##### Conclusion:\n",
    "While the newly generated text continues to exhibit some resemblance to the style of Rihanna's songs, there are still areas for improvement in terms of coherence and accuracy. Experimenting with different hyperparameters and possibly using more advanced techniques could help enhance the model's performance and produce more coherent and stylistically accurate generated text. Additionally, providing a larger and more diverse training dataset may also improve the model's ability to capture the intricacies of song lyrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
